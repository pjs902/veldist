# End-to-End Examples

This page provides complete, runnable examples demonstrating how to use `veldist` for various scientific use cases.

```{note}
The example images shown below can be regenerated by running:
\`\`\`bash
cd docs
python generate_example_images.py
\`\`\`
This script will create all the example plots in the `docs/images/` directory.
```

## Example 1: Basic Gaussian Distribution Recovery

This example demonstrates the simplest use case: recovering a Gaussian velocity distribution from noisy observations.

```python
import numpy as np
import matplotlib.pyplot as plt
from veldist import KinematicSolver

# Set random seed for reproducibility
np.random.seed(42)

# ========================================
# 1. Generate Synthetic Data
# ========================================

# True distribution parameters
true_mean = 0.0  # km/s
true_std = 10.0  # km/s
n_stars = 500

# Generate true velocities from a Gaussian
true_velocities = np.random.normal(true_mean, true_std, n_stars)

# Add observational errors
measurement_errors = np.ones(n_stars) * 2.0  # km/s
observed_velocities = true_velocities + np.random.normal(0, measurement_errors)

print(f"Generated {n_stars} stars")
print(f"True mean: {true_mean:.1f} km/s, True std: {true_std:.1f} km/s")
print(f"Observed mean: {np.mean(observed_velocities):.1f} km/s")
print(f"Observed std: {np.std(observed_velocities):.1f} km/s")

# ========================================
# 2. Set Up the Solver
# ========================================

solver = KinematicSolver()

# Define the velocity grid
# - center: center of the grid
# - width: total width of the grid
# - n_bins: number of histogram bins
solver.setup_grid(center=0.0, width=100.0, n_bins=50)

print(f"\nGrid setup: {solver.grid['n_bins']} bins")
print(f"Bin width: {solver.grid['width']:.2f} km/s")

# ========================================
# 3. Add Data
# ========================================

solver.add_data(vel=observed_velocities, err=measurement_errors)

# ========================================
# 4. Run MCMC Inference
# ========================================

samples = solver.run(
    num_warmup=500,   # Burn-in samples
    num_samples=1000, # Posterior samples
    gpu=False         # Set to True if GPU available
)

print(f"\nInference complete!")
print(f"Posterior samples shape: {samples['intrinsic_pdf'].shape}")
print(f"Smoothness sigma: {np.mean(samples['smoothness_sigma']):.4f} ± "
      f"{np.std(samples['smoothness_sigma']):.4f}")

# ========================================
# 5. Analyze Results
# ========================================

from veldist import compute_moments

moments = compute_moments(samples['intrinsic_pdf'], solver.grid['centers'])

print(f"\n=== Results ===")
print(f"Inferred mean: {moments['mean'][0]:.2f} ± {moments['mean'][1]:.2f} km/s")
print(f"Inferred std:  {moments['std'][0]:.2f} ± {moments['std'][1]:.2f} km/s")
print(f"True mean:     {true_mean:.2f} km/s")
print(f"True std:      {true_std:.2f} km/s")

# ========================================
# 6. Visualize
# ========================================

fig, axes = plt.subplots(1, 2, figsize=(12, 4))

# Left panel: Data histogram vs inferred distribution
ax = axes[0]
ax.hist(observed_velocities, bins=30, density=True, alpha=0.5, 
        label='Observed Data', color='gray')
solver.plot_result(ax=ax, true_intrinsic=true_velocities)
ax.set_title('Recovered Distribution')
ax.legend()

# Right panel: Posterior samples of smoothness parameter
ax = axes[1]
ax.hist(samples['smoothness_sigma'], bins=30, alpha=0.7, color='steelblue')
ax.axvline(np.mean(samples['smoothness_sigma']), color='red', 
           linestyle='--', label='Mean')
ax.set_xlabel('Smoothness σ')
ax.set_ylabel('Frequency')
ax.set_title('Inferred Smoothness Parameter')
ax.legend()

plt.tight_layout()
plt.savefig('docs/images/example_gaussian.png', dpi=150)
plt.show()
```

### Expected Output

**Console output:**

```
Generated 500 stars
True mean: 0.0 km/s, True std: 10.0 km/s
Observed mean: 0.1 km/s
Observed std: 10.2 km/s

Grid setup: 50 bins
Bin width: 2.00 km/s
Computing Design Matrix for 500 stars...
Matrix ready. Shape: (500, 50)

Starting NUTS MCMC...
Inference Complete.

Inference complete!
Posterior samples shape: (1000, 50)
Smoothness sigma: 0.0234 ± 0.0089

=== Results ===
Inferred mean: 0.12 ± 0.45 km/s
Inferred std:  9.98 ± 0.35 km/s
True mean:     0.00 km/s
True std:      10.00 km/s
```

**Visual output:**

![Example 1: Gaussian Recovery](images/example_gaussian.png)

The left panel shows the inferred intrinsic distribution (green) compared to the true distribution (black dashed) and observed data histogram. The green shaded region represents the 68% credible interval. The right panel shows the posterior distribution of the smoothness hyperparameter, demonstrating that the optimal regularization is learned from the data.

---

## Example 2: Deconvolution with High Noise

This example demonstrates `veldist`'s key strength: recovering the intrinsic distribution when measurement errors are comparable to or larger than the intrinsic spread.

```python
import numpy as np
import matplotlib.pyplot as plt
from veldist import KinematicSolver, compute_moments

np.random.seed(123)

# ========================================
# 1. Generate Data with High Noise
# ========================================

# Narrow intrinsic distribution
true_mean = 0.0
true_std = 5.0  # Small intrinsic width
n_stars = 1000

true_velocities = np.random.normal(true_mean, true_std, n_stars)

# Large measurement errors (comparable to intrinsic width)
measurement_errors = np.ones(n_stars) * 7.0  # Larger than intrinsic!
observed_velocities = true_velocities + np.random.normal(0, measurement_errors)

# Compare spreads
obs_std = np.std(observed_velocities)
print(f"True intrinsic dispersion: {true_std:.1f} km/s")
print(f"Measurement error:         {measurement_errors[0]:.1f} km/s")
print(f"Observed dispersion:       {obs_std:.1f} km/s")
print(f"Noise is {measurement_errors[0]/true_std:.1f}x larger than signal!")

# ========================================
# 2. Run Inference
# ========================================

solver = KinematicSolver()
solver.setup_grid(center=0.0, width=100.0, n_bins=50)
solver.add_data(vel=observed_velocities, err=measurement_errors)

samples = solver.run(num_warmup=500, num_samples=1000, gpu=False)

# ========================================
# 3. Compare: Naive vs Deconvolved
# ========================================

moments = compute_moments(samples['intrinsic_pdf'], solver.grid['centers'])

naive_std = np.std(observed_velocities)
deconvolved_std = moments['std'][0]
true_std_val = true_std

print(f"\n=== Dispersion Recovery ===")
print(f"Naive (observed):    {naive_std:.2f} km/s")
print(f"Deconvolved:         {deconvolved_std:.2f} ± {moments['std'][1]:.2f} km/s")
print(f"True intrinsic:      {true_std_val:.2f} km/s")
print(f"\nError reduction: {abs(naive_std - true_std_val):.2f} → "
      f"{abs(deconvolved_std - true_std_val):.2f} km/s")

# ========================================
# 4. Visualization
# ========================================

fig, axes = plt.subplots(1, 3, figsize=(15, 4))

# Panel 1: Observed data
ax = axes[0]
ax.hist(observed_velocities, bins=40, density=True, alpha=0.6, 
        color='gray', label='Observed')
x = np.linspace(-30, 30, 200)
ax.plot(x, np.exp(-0.5*x**2/naive_std**2)/(naive_std*np.sqrt(2*np.pi)), 
        'k--', label=f'Gaussian fit (σ={naive_std:.1f})')
ax.set_xlabel('Velocity (km/s)')
ax.set_ylabel('Density')
ax.set_title('Observed Data (Naive)')
ax.legend()

# Panel 2: Deconvolved result
ax = axes[1]
solver.plot_result(ax=ax, true_intrinsic=true_velocities)
ax.set_title('Deconvolved Distribution')

# Panel 3: Comparison
ax = axes[2]
bins = solver.grid['centers']
pdf_samples = samples['intrinsic_pdf'] / solver.grid['width']
mean_pdf = np.mean(pdf_samples, axis=0)

true_pdf = np.exp(-0.5*bins**2/true_std**2)/(true_std*np.sqrt(2*np.pi))

ax.plot(bins, mean_pdf, 'g-', linewidth=2, label='Inferred')
ax.plot(bins, true_pdf, 'k--', linewidth=2, label='True')
ax.fill_between(bins, 
                np.percentile(pdf_samples, 16, axis=0),
                np.percentile(pdf_samples, 84, axis=0),
                alpha=0.3, color='green')
ax.set_xlabel('Velocity (km/s)')
ax.set_ylabel('Probability Density')
ax.set_title('Deconvolution Recovers True Width')
ax.legend()

plt.tight_layout()
plt.savefig('docs/images/example_deconvolution.png', dpi=150)
plt.show()
```

### Visual Output

![Example 2: Deconvolution with High Noise](images/example_deconvolution.png)

This figure demonstrates the power of deconvolution. The left panel shows the naive observed distribution (gray histogram) which is broadened by large measurement errors. The middle panel shows the deconvolved intrinsic distribution, successfully recovering the narrower true width. The right panel directly compares the inferred distribution (green) to the true underlying distribution (black dashed), showing excellent agreement despite noise being larger than the signal.

---

## Example 3: Multi-Component System

This example shows how `veldist` can recover complex, non-Gaussian distributions with multiple components—common in stellar systems with multiple populations.

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.signal import find_peaks
from veldist import KinematicSolver

np.random.seed(456)

# ========================================
# 1. Create Multi-Component Distribution
# ========================================

# Simulate 3 stellar populations
# Component 1: Cold disk (narrow, high weight)
n1 = 600
mean1, std1 = 0.0, 5.0

# Component 2: Warm disk (broader)
n2 = 300
mean2, std2 = 0.0, 15.0

# Component 3: Halo (offset, broad)
n3 = 100
mean3, std3 = 30.0, 20.0

# Generate samples
v1 = np.random.normal(mean1, std1, n1)
v2 = np.random.normal(mean2, std2, n2)
v3 = np.random.normal(mean3, std3, n3)

true_velocities = np.concatenate([v1, v2, v3])
n_stars = len(true_velocities)

# Heteroscedastic errors (vary by star)
errors = np.concatenate([
    np.random.uniform(2, 4, n1),   # Good measurements
    np.random.uniform(3, 6, n2),   # Medium quality
    np.random.uniform(5, 10, n3)   # Poor quality
])

observed_velocities = true_velocities + np.random.normal(0, errors)

print(f"Component 1 (cold disk): {n1} stars, σ={std1} km/s")
print(f"Component 2 (warm disk): {n2} stars, σ={std2} km/s")
print(f"Component 3 (halo):      {n3} stars, μ={mean3}, σ={std3} km/s")
print(f"Total: {n_stars} stars")
print(f"Error range: {errors.min():.1f} - {errors.max():.1f} km/s")

# ========================================
# 2. Run Inference
# ========================================

solver = KinematicSolver()
solver.setup_grid(center=10.0, width=120.0, n_bins=60)
solver.add_data(vel=observed_velocities, err=errors)

samples = solver.run(num_warmup=500, num_samples=1000, gpu=False)

# ========================================
# 3. Identify Components
# ========================================

pdf_density = samples['intrinsic_pdf'] / solver.grid['width']
mean_pdf = np.mean(pdf_density, axis=0)
centers = solver.grid['centers']

# Find peaks in the inferred distribution
peaks, properties = find_peaks(mean_pdf, distance=5, prominence=0.001)

print(f"\n=== Detected {len(peaks)} components ===")
for i, peak_idx in enumerate(peaks, 1):
    peak_vel = centers[peak_idx]
    peak_height = mean_pdf[peak_idx]
    print(f"Component {i}: velocity ≈ {peak_vel:.1f} km/s, "
          f"height = {peak_height:.4f}")

# ========================================
# 4. Visualization
# ========================================

fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# Panel 1: Observed data colored by component
ax = axes[0, 0]
ax.hist(v1 + np.random.normal(0, errors[:n1]), bins=30, alpha=0.5, 
        label='Cold disk', density=True)
ax.hist(v2 + np.random.normal(0, errors[n1:n1+n2]), bins=30, alpha=0.5, 
        label='Warm disk', density=True)
ax.hist(v3 + np.random.normal(0, errors[n1+n2:]), bins=30, alpha=0.5, 
        label='Halo', density=True)
ax.set_xlabel('Observed Velocity (km/s)')
ax.set_ylabel('Density')
ax.set_title('Observed Data (with errors)')
ax.legend()

# Panel 2: Inferred distribution
ax = axes[0, 1]
solver.plot_result(ax=ax, true_intrinsic=true_velocities)
ax.plot(centers[peaks], mean_pdf[peaks], 'r*', markersize=15, 
        label='Detected Peaks')
ax.set_title(f'Inferred Distribution ({len(peaks)} components)')
ax.legend()

# Panel 3: Error distribution
ax = axes[1, 0]
ax.scatter(observed_velocities, errors, alpha=0.3, s=10)
ax.set_xlabel('Observed Velocity (km/s)')
ax.set_ylabel('Measurement Error (km/s)')
ax.set_title('Heteroscedastic Errors')
ax.grid(True, alpha=0.3)

# Panel 4: Smoothness parameter evolution
ax = axes[1, 1]
ax.plot(samples['smoothness_sigma'], alpha=0.5, linewidth=0.5)
ax.axhline(np.mean(samples['smoothness_sigma']), color='red', 
           linestyle='--', label='Mean')
ax.set_xlabel('MCMC Sample')
ax.set_ylabel('Smoothness σ')
ax.set_title('Smoothness Parameter Trace')
ax.legend()

plt.tight_layout()
plt.savefig('docs/images/example_multicomponent.png', dpi=150)
plt.show()
```

### Visual Output

![Example 3: Multi-Component System](images/example_multicomponent.png)

**Top left:** Individual components shown in the observed data. **Top right:** The inferred distribution successfully identifies multiple peaks (marked with red stars) corresponding to the three stellar populations. **Bottom left:** The heteroscedastic measurement errors plotted against velocity, showing how error quality varies across the dataset. **Bottom right:** MCMC trace of the smoothness parameter, indicating good convergence.

---

## Example 4: Real-World Workflow with Quality Checks

This example demonstrates a complete analysis pipeline with diagnostic checks and uncertainty quantification.

```python
import numpy as np
import matplotlib.pyplot as plt
from veldist import KinematicSolver, compute_moments

np.random.seed(789)

# ========================================
# 1. Simulate "Real" Data
# ========================================

# Asymmetric distribution (e.g., infalling stream)
def asymmetric_pdf(x, center=0, width=10, skew=0.5):
    """Skewed Gaussian for realistic asymmetry."""
    from scipy.stats import skewnorm
    return skewnorm.pdf(x, skew, loc=center, scale=width)

# Generate from asymmetric distribution
from scipy.stats import skewnorm
true_velocities = skewnorm.rvs(a=2, loc=5, scale=8, size=400)

# Realistic error model (depends on magnitude, etc.)
# Better measurements for slow-moving stars
errors = 2.0 + 0.1 * np.abs(true_velocities) + np.random.uniform(0, 1, 400)
observed_velocities = true_velocities + np.random.normal(0, errors)

print("=== Dataset Summary ===")
print(f"N stars: {len(true_velocities)}")
print(f"Velocity range: {observed_velocities.min():.1f} to "
      f"{observed_velocities.max():.1f} km/s")
print(f"Median error: {np.median(errors):.2f} km/s")
print(f"Error range: {errors.min():.1f} - {errors.max():.1f} km/s")

# ========================================
# 2. Pre-Analysis Checks
# ========================================

# Check for outliers
q1, q3 = np.percentile(observed_velocities, [25, 75])
iqr = q3 - q1
outliers = (observed_velocities < q1 - 3*iqr) | (observed_velocities > q3 + 3*iqr)
print(f"\nOutliers detected: {outliers.sum()} stars")

if outliers.sum() > 0:
    print("Consider removing or flagging outliers")

# Error budget
signal = np.std(observed_velocities)
noise = np.median(errors)
snr = signal / noise
print(f"\nSignal-to-noise: {snr:.2f}")
if snr < 2:
    print("Warning: Low SNR may lead to high uncertainties")

# ========================================
# 3. Grid Selection
# ========================================

# Choose grid based on data range
data_min, data_max = np.percentile(observed_velocities, [1, 99])
data_range = data_max - data_min
grid_center = np.median(observed_velocities)
grid_width = data_range * 1.5  # Add 50% buffer

print(f"\n=== Grid Setup ===")
print(f"Data 99% range: {data_min:.1f} to {data_max:.1f} km/s")
print(f"Grid center: {grid_center:.1f} km/s")
print(f"Grid width: {grid_width:.1f} km/s")

# Choose number of bins
# Rule of thumb: ~3-5 bins per expected feature width
typical_error = np.median(errors)
n_bins = int(grid_width / (2 * typical_error))
n_bins = np.clip(n_bins, 30, 100)  # Keep reasonable
print(f"Number of bins: {n_bins}")

# ========================================
# 4. Run Inference
# ========================================

solver = KinematicSolver()
solver.setup_grid(center=grid_center, width=grid_width, n_bins=n_bins)
solver.add_data(vel=observed_velocities, err=errors)

samples = solver.run(num_warmup=1000, num_samples=2000, gpu=False)

# ========================================
# 5. Convergence Diagnostics
# ========================================

print("\n=== Convergence Diagnostics ===")

# Check smoothness parameter convergence
smooth_samples = samples['smoothness_sigma']
first_half = smooth_samples[:len(smooth_samples)//2]
second_half = smooth_samples[len(smooth_samples)//2:]

mean_1 = np.mean(first_half)
mean_2 = np.mean(second_half)
std_pooled = np.sqrt((np.var(first_half) + np.var(second_half)) / 2)

drift = abs(mean_1 - mean_2) / std_pooled
print(f"Smoothness drift between halves: {drift:.2f} sigma")
if drift > 0.5:
    print("Warning: Possible non-convergence, consider more samples")
else:
    print("✓ Smoothness parameter appears converged")

# Check PDF sample stability
pdf_var = np.var(samples['intrinsic_pdf'], axis=0)
high_var_bins = np.sum(pdf_var > np.median(pdf_var) * 5)
print(f"High-variance bins: {high_var_bins} / {n_bins}")

# ========================================
# 6. Results and Uncertainties
# ========================================

moments = compute_moments(samples['intrinsic_pdf'], solver.grid['centers'])

print("\n=== Results ===")
print(f"Mean velocity: {moments['mean'][0]:.2f} ± {moments['mean'][1]:.2f} km/s")
print(f"Dispersion:    {moments['std'][0]:.2f} ± {moments['std'][1]:.2f} km/s")
print(f"Skewness:      {moments['skewness'][0]:.2f} ± {moments['skewness'][1]:.2f}")
print(f"Kurtosis:      {moments['kurtosis'][0]:.2f} ± {moments['kurtosis'][1]:.2f}")

# Credible intervals
centers = solver.grid['centers']
pdf_density = samples['intrinsic_pdf'] / solver.grid['width']
lower_90 = np.percentile(pdf_density, 5, axis=0)
upper_90 = np.percentile(pdf_density, 95, axis=0)
relative_unc = (upper_90 - lower_90) / (np.mean(pdf_density, axis=0) + 1e-10)

print(f"\nMedian relative uncertainty: {np.median(relative_unc):.1%}")

# ========================================
# 7. Publication-Quality Visualization
# ========================================

fig = plt.figure(figsize=(14, 10))
gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)

# Main result
ax1 = fig.add_subplot(gs[0:2, :])
solver.plot_result(ax=ax1, true_intrinsic=true_velocities)
ax1.hist(observed_velocities, bins=50, density=True, alpha=0.3, 
         color='gray', label='Observed (with errors)', zorder=0)
ax1.set_xlabel('Velocity (km/s)', fontsize=12)
ax1.set_ylabel('Probability Density', fontsize=12)
ax1.set_title('Inferred Line-of-Sight Velocity Distribution', fontsize=14)
ax1.legend(fontsize=10)
ax1.grid(True, alpha=0.2)

# Convergence trace
ax2 = fig.add_subplot(gs[2, 0])
ax2.plot(smooth_samples, alpha=0.7, linewidth=0.5)
ax2.axhline(np.mean(smooth_samples), color='red', linestyle='--')
ax2.axhline(np.percentile(smooth_samples, 16), color='red', 
            linestyle=':', alpha=0.5)
ax2.axhline(np.percentile(smooth_samples, 84), color='red', 
            linestyle=':', alpha=0.5)
ax2.set_xlabel('MCMC Iteration', fontsize=10)
ax2.set_ylabel('Smoothness σ', fontsize=10)
ax2.set_title('Parameter Trace', fontsize=11)

# Uncertainty map
ax3 = fig.add_subplot(gs[2, 1])
ax3.fill_between(centers, lower_90, upper_90, alpha=0.3, color='steelblue')
ax3.plot(centers, np.mean(pdf_density, axis=0), 'b-', linewidth=1.5)
ax3.set_xlabel('Velocity (km/s)', fontsize=10)
ax3.set_ylabel('Probability Density', fontsize=10)
ax3.set_title('90% Credible Interval', fontsize=11)
ax3.grid(True, alpha=0.2)

plt.savefig('docs/images/example_complete_workflow.png', dpi=150, bbox_inches='tight')
plt.show()

print("\n✓ Analysis complete. Results saved to 'docs/images/example_complete_workflow.png'")
```

### Visual Output

![Example 4: Complete Workflow](images/example_complete_workflow.png)

**Top panel:** Publication-quality plot showing the inferred distribution with credible intervals overlaid on the observed data. **Bottom left:** MCMC trace plot showing convergence of the smoothness parameter with 68% credible interval bands. **Bottom right:** 90% credible interval visualization showing where the inference is most/least certain.
